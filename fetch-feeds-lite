import feedparser
import json
from datetime import datetime, timedelta
import time

rss_feeds = ['http://example.com/rss/feed1.xml', 'http://example.com/rss/feed2.xml']
keywords = ['keyword1', 'keyword2', 'keyword3']

def scrape_and_save(feed):
    matching_entries = []

    for entry in feed.entries:
        if any(keyword in entry.title.lower() or keyword in entry.summary.lower() for keyword in keywords):
            matching_entries.append({
                'title': entry.title,
                'summary': entry.summary,
                'link': entry.link
            })

    file_prefix = f'matching_entries_{feed.feed.title}'
    
    with open(f'{file_prefix}.txt', 'w') as txt_file, open(f'{file_prefix}.json', 'w') as json_file:
        for entry in matching_entries:
            txt_file.write(f"Title: {entry['title']}\nSummary: {entry['summary']}\nLink: {entry['link']}\n\n")
        json.dump(matching_entries, json_file, indent=4)

def run_daily_scraping():
    start_time = datetime.now().replace(hour=0, minute=0, second=0) + timedelta(days=1)
    delay = (start_time - datetime.now()).total_seconds()
    time.sleep(delay)

    for feed_url in rss_feeds:
        feed = feedparser.parse(feed_url)
        scrape_and_save(feed)

    exit()

run_daily_scraping()
